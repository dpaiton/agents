# Bias Awareness Guide

This document defines the five evaluation biases that threaten scoring validity in LLM-as-judge pipelines. Each bias includes a definition, explanation of why it matters, concrete procedural mitigations, and a detection method.

These mitigations are **procedural, not aspirational**. Each one is a step you execute, not an attitude you adopt (P5: Deterministic Infrastructure). The system is designed so that biases are caught by process, not by willpower.

---

## 1. Position Bias

### Definition

The tendency to favor the response that appears first (or last) in a pairwise comparison, regardless of quality. LLMs are particularly susceptible to favoring the first response presented.

### Why It Matters

In pairwise evaluation (comparing Response A vs Response B), the judge may systematically prefer whichever response occupies position A. This means evaluation outcomes depend on presentation order rather than actual quality, producing unreliable rankings.

### Concrete Mitigation Steps

1. **Swap A/B order.** For every pairwise comparison, run the evaluation twice: once with the original order (A, B) and once with the reversed order (B, A).
2. **Compare verdicts.** Record the winner from each ordering independently.
3. **Flag disagreements.** If the winner changes when the order is swapped, flag the comparison as position-sensitive.
4. **Resolve flagged cases.** Position-sensitive comparisons are scored as a tie, or escalated to a human reviewer. Never accept a position-biased result as a clean win.
5. **Log swap rates.** Track what percentage of comparisons produce disagreements when swapped. A swap disagreement rate above 15% indicates systemic position bias in the judge prompt and requires prompt revision.

### Detection Method

Calculate the **positional agreement rate**: the percentage of evaluations where the winner is the same regardless of presentation order. Track this metric per judge prompt version. If agreement drops below 85%, the judge prompt needs revision.

```
positional_agreement = (concordant_pairs / total_pairs) * 100
if positional_agreement < 85:
    flag_for_prompt_revision()
```

---

## 2. Verbosity Bias

### Definition

The tendency to prefer longer, more detailed responses over shorter ones, even when the longer response contains padding, repetition, or irrelevant information and the shorter response is more precise.

### Why It Matters

Agents that produce verbose output will be scored higher than agents that produce concise, correct output. This incentivizes padding and discourages the UNIX philosophy of doing one thing well (P8). It also inflates scores for responses that bury errors in walls of text.

### Concrete Mitigation Steps

1. **Evaluate substance, not length.** The judge prompt must explicitly instruct: "A shorter response that is correct and complete scores higher than a longer response that adds unnecessary detail."
2. **Penalize padding.** The rubric must treat unnecessary repetition, filler phrases, and restating the question as negative signals. If the response repeats the same point in different words, deduct from Code Quality (duplication).
3. **Normalize before scoring.** When comparing two responses, note their lengths. If one is more than 2x the length of the other, explicitly ask: "Does the additional length add substantive value, or is it padding?"
4. **Include concise gold-standard examples.** Calibration examples (see [judge-calibration.md](judge-calibration.md)) should include at least one high-scoring response that is notably short and one low-scoring response that is notably long.
5. **Track length-score correlation.** Compute the correlation between response length (token count) and total score. A Pearson correlation above 0.5 suggests verbosity bias.

### Detection Method

Compute the **length-score correlation** across all evaluations. If longer responses consistently score higher, the judge is exhibiting verbosity bias.

```
correlation = pearson(response_lengths, total_scores)
if correlation > 0.5:
    flag_verbosity_bias()
```

---

## 3. Self-Enhancement Bias

### Definition

The tendency of an LLM judge to prefer responses generated by itself or by its own model family over responses from other models.

### Why It Matters

If the judge knows (or can infer) which model generated a response, it may unconsciously favor that model's style, phrasing, or reasoning patterns. This makes cross-model comparison invalid and undermines the purpose of evaluation — which is to measure quality, not model identity.

### Concrete Mitigation Steps

1. **Blind the evaluator to model identity.** Never include the model name, model version, or any metadata that reveals which model generated the response. Strip all such information before passing to the judge.
2. **Use neutral labels.** Refer to responses as "Response A" and "Response B", or "Submission 1" and "Submission 2". Never use model names as labels.
3. **Strip model-identifying artifacts.** Some models have distinctive output patterns (e.g., specific disclaimer phrases, formatting habits, or reasoning markers). Normalize these before evaluation when possible.
4. **Rotate judge models.** When feasible, use a different model family for judging than for generation. If the coding agent uses Model X, prefer Model Y as judge.
5. **Audit for stylistic preference.** Periodically test whether the judge scores its own model's outputs higher than equivalent outputs from other models by submitting semantically identical responses with different surface styles.

### Detection Method

Run a **blind identity test**: submit pairs of semantically equivalent responses where one uses the judge model's characteristic style and the other uses a neutral or different style. If the judge consistently prefers its own style, self-enhancement bias is present.

```
own_style_wins = count(judge_prefers_own_style)
other_style_wins = count(judge_prefers_other_style)
if own_style_wins / total > 0.6:
    flag_self_enhancement_bias()
```

---

## 4. Authority Bias

### Definition

The tendency to rate a response higher because it appears to come from a more authoritative source, or because it uses authoritative language (citing documentation, using confident tone, referencing well-known tools), regardless of whether the content is actually correct.

### Why It Matters

An agent that confidently states an incorrect answer with citations will be scored higher than an agent that gives the correct answer with appropriate uncertainty. This rewards hallucinated confidence and punishes honest uncertainty — the opposite of good engineering (P16: Permission to Fail).

### Concrete Mitigation Steps

1. **Evaluate content, not attribution.** The judge prompt must explicitly instruct: "Ignore citations, references to documentation, or appeals to authority. Score based on whether the code works and the reasoning is sound."
2. **Verify claims independently.** When a response cites a source, the judge should not grant credit for the citation itself. Credit is granted only if the underlying claim is correct.
3. **Do not penalize uncertainty.** A response that says "I'm not sure about X, but here's my best approach" should not be scored lower than a response that states the same approach with false confidence — assuming the approach itself is equally valid.
4. **Normalize confidence language.** Instruct the judge to disregard phrases like "definitely", "obviously", "as documented in", and "best practice" as scoring signals. These are stylistic, not substantive.
5. **Test with planted authority.** Periodically submit pairs of responses where the incorrect response uses authoritative language and the correct response uses uncertain language. If the judge prefers the authoritative-but-wrong response, authority bias is present.

### Detection Method

Run a **planted authority test**: create response pairs where one is correct but hedging and the other is incorrect but authoritative. Track how often the judge selects the authoritative-but-wrong option.

```
authority_wrong_preferred = count(judge_picks_confident_incorrect)
if authority_wrong_preferred / total_planted_tests > 0.2:
    flag_authority_bias()
```

---

## 5. Format Bias

### Definition

The tendency to score responses higher because they use attractive formatting (markdown headers, bullet lists, code blocks, tables) rather than because the content is substantively better.

### Why It Matters

Agents may learn to "game" evaluation by producing well-formatted but shallow responses. A beautifully formatted wrong answer should never outscore a plain-text correct answer. Format is a secondary concern; substance is the primary one.

### Concrete Mitigation Steps

1. **Evaluate substance over formatting quality.** The judge prompt must explicitly instruct: "Formatting is not a scoring criterion. A correct answer in plain text scores the same as a correct answer with markdown formatting."
2. **Do not reward decorative formatting.** Headers, horizontal rules, emoji, and other visual elements should not influence scores unless the spec specifically requires structured output.
3. **Score content extraction, not presentation.** When evaluating, mentally (or programmatically) strip formatting and evaluate the underlying claims, code, and reasoning.
4. **Include format-varied calibration examples.** Gold-standard examples should include at least one high-scoring response with minimal formatting and one low-scoring response with heavy formatting. This anchors the judge's expectations.
5. **Penalize format-over-substance.** If a response uses formatting to mask lack of content (e.g., many headers with little under each), this is a negative signal under Code Quality (duplication/padding).

### Detection Method

Run a **format stripping test**: take a set of responses, create plain-text versions by stripping all markdown formatting, and re-score. Compare scores before and after stripping. If scores drop significantly after stripping, format bias is present.

```
score_diff = mean(formatted_scores) - mean(stripped_scores)
if score_diff > 0.5:  # average score difference per criterion
    flag_format_bias()
```

---

## Bias Mitigation Checklist

Before deploying any judge prompt to production, verify:

- [ ] Pairwise evaluations use A/B order swapping
- [ ] Judge prompt explicitly instructs "evaluate substance, not length"
- [ ] Model identity is stripped from all inputs to the judge
- [ ] Judge prompt instructs to ignore authoritative language
- [ ] Judge prompt instructs to ignore formatting quality
- [ ] Calibration examples include short/high-score and long/low-score pairs
- [ ] Positional agreement rate is measured and above 85%
- [ ] Length-score correlation is measured and below 0.5

### Checklist Execution Policy

This checklist incurs token expenditure each time it runs, which increases cost. To balance quality assurance against cost, the checklist runs with a **tunable probability** that is configurable per deployment:

- **Probability parameter:** Set the probability (0% to 100%) that the checklist runs on any given evaluation. This can be set to 0% to disable automated checks entirely, or 100% for full coverage during calibration periods.
- **Manual trigger:** The checklist can always be run manually via CLI tooling (e.g., `calibrate --bias-check`), regardless of the probability setting.
- **Default assumption when skipped:** When the probability check skips the checklist, the system assumes judges are operating correctly. No bias flags are raised and evaluation proceeds normally.

This is a cost-conscious design: the checklist exists as a safety net, not a gate on every evaluation. During active development or after prompt changes, increase the probability. During stable operation with well-calibrated judges, reduce it to minimize unnecessary token expenditure.

## References

- [Code Review Rubric](rubrics.md) — The scoring criteria the judge applies
- [Judge Calibration](judge-calibration.md) — Gold-standard examples and calibration workflow
